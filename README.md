# Aryabhatta 
This project is a fault-aware AI prompt processing service built with FastAPI.
It accepts user prompts, applies an agent-specific system prompt, calls a Hugging Face LLM, and atomically stores both the user prompt and AI response only when the LLM call succeeds.

The system is designed to be transaction-safe, network-failure tolerant, and cleanly extensible for production workloads.

At the end what you will see is a basic version of relevance 

I will be adding tools support as well ... 

## How to run this project? 
### Pre-requisites : 
- You should already have the hugging face auth token if you don't have it then you can go to this 
- ![generate_access_token](docs_images/generate_access_token.png)
- From the list of options select Access token option to generate the access token fo your application
- After generating access token you will see something like this 
- ![access_token_page](docs_images/access_token_page.png)
- After that create a .env file in the root project directory ```.env```
- Add the token like this 
```bash
HUGGING_FACE_AUTH_TOKEN = your_hf_auth_token
HF_API_URL = https://router.huggingface.co/v1/chat/completions

DB_CONNECTION_STRING = postgresql+psycopg://user_name:password@localhost:5432/db_name
```
### After setting up the project's env file 
go to this directory
```bash
cd /home/aditya/github/agentic_ai_cloud
```
type this command to run your uvicorn asgi server
```bash
uvicorn app.main:app --reload
```
This command will run your back-end server in localhost ip but if you want to run your back-end on another ip then simply use the command below
```bash
uvicorn app.main:app --host <yout_laptop's_ip> --port <your_port_number> --reload
```
example : 
```bash
uvicorn app.main:app --host 192.168.1.204 --port 8000 --reload
```
This will allow you to serve your server on a local network over wifi if you want.

## Environment variables
You will have to set the Environment variables ```.env``` related to hugging face api and postgreSql
```bash
HUGGING_FACE_AUTH_TOKEN = hf_mLJifvMK1234567890trAlknGiJDVnVKmL
HF_API_URL = https://router.huggingface.co/v1/chat/completions

DB_CONNECTION_STRING = postgresql://<user_name>:<password>@<db_ip>:5432/<db_name>
```
the ```.env``` file must be at the root project directory

## Managing database migrations 
For this project I am using SQL alchemy to :
- Define the database tables
- Insert , Update , Fetch and Delete the records from the table
and alembic to : 
- Manage the database migrations and schema changes like django

In order to manage db_migrations in alembic 
- Step 1 : go to this dir /home/aditya/github/agentic_ai_cloud/app
```bash
cd /home/aditya/github/agentic_ai_cloud/app
```
- Step 2 : Initialize alembic
```bash
alembic init alembic
```
- Step 3 : Make changes to this ```app/alembic/env.py``` file. I have already made changes to this file you don't have to do it for this project. I am only telling your this so that if you ever want to use alembic to manage your database migrations in fast-api or in flask you can use the code below. 

One thing to note is that the code below will find your .env file and extract the database connection string from there 
```python
from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context

import os
import sys
from logging.config import fileConfig

from decouple import config as env_config

from app.database.base import Base
from app.models.db_table_models.ai_agent_table import AIAgentName

from sqlalchemy import create_engine

# ---- ADD PROJECT ROOT TO PYTHON PATH ----
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(BASE_DIR)

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = Base.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    DATABASE_URL = env_config("DB_CONNECTION_STRING")
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = create_engine(
        DATABASE_URL,
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```
- Step 4 : use the commands below to run the migrations (This particular command is to create the table before db operation)
```bash
cd /home/aditya/github/agentic_ai_cloud
alembic revision --autogenerate -m "create ai_agent_table"
alembic -c alembic.ini upgrade head
```

**NOTE:** Make sure that your project structure looks something like this 
```bash
.
â”œâ”€â”€ alembic
â”‚   â”œâ”€â”€ env.py
â”‚   â”œâ”€â”€ README
â”‚   â”œâ”€â”€ script.py.mako
â”‚   â””â”€â”€ versions
â”‚       â”œâ”€â”€ 027e8d073bf8_create_ai_agent_table.py
â”‚       â”œâ”€â”€ 20178cf2ef3e_create_user_prompt_table.py
â”‚       â”œâ”€â”€ 37463c85173b_create_tables.py
â”‚       â”œâ”€â”€ 4778aedf35a2_create_ai_agent_table.py
â”‚       â”œâ”€â”€ 53e83fac2395_create_system_prompt_table.py
â”‚       â”œâ”€â”€ 5cffdaf15574_create_llm_prompt_response_table.py
â”‚       â”œâ”€â”€ 6146d779b733_create_system_prompt_table.py
â”‚       â”œâ”€â”€ b20ac25ffcd0_create_system_prompt_table.py
â”‚       â”œâ”€â”€ c3c1c2bd75c0_create_system_prompt_table.py
â”‚       â”œâ”€â”€ c7af7dbf402a_create_user_prompt_table.py
â”‚       â”œâ”€â”€ ff1251aeb20c_create_user_prompt_table.py
â”œâ”€â”€ alembic.ini
â”œâ”€â”€ app
â”‚   â”œâ”€â”€ apis
â”‚   â”‚   â”œâ”€â”€ agent_api.py
â”‚   â”‚   â”œâ”€â”€ hugging_face_api.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ prompt_apis.py
â”‚   â”œâ”€â”€ configs
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ controllers
â”‚   â”‚   â”œâ”€â”€ agent_controllers.py
â”‚   â”‚   â”œâ”€â”€ hugging_face_ai_model_controllers.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ prompt_controllers.py
â”‚   â”œâ”€â”€ database
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ db_session.py
â”‚   â”‚   â”œâ”€â”€ db_transaction_exception_handler.py
â”‚   â”œâ”€â”€ dependencies
â”‚   â”‚   â”œâ”€â”€ controller_dependencies.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logs
â”‚   â”‚   â”œâ”€â”€ debug
â”‚   â”‚   â”‚   â””â”€â”€ debug.log
â”‚   â”‚   â”œâ”€â”€ error
â”‚   â”‚   â”‚   â””â”€â”€ error.log
â”‚   â”‚   â””â”€â”€ info
â”‚   â”‚       â””â”€â”€ info.log
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ models
â”‚   â”‚   â”œâ”€â”€ api_request_response_model
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ request_models.py
â”‚   â”‚   â”‚   â””â”€â”€ response_models.py
â”‚   â”‚   â”œâ”€â”€ class_request_model
â”‚   â”‚   â”‚   â”œâ”€â”€ class_request_model.py
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ class_return_model
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ services_class_response_models.py
â”‚   â”‚   â”œâ”€â”€ db_table_models
â”‚   â”‚   â”‚   â”œâ”€â”€ ai_agent_table.py
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_prompt_response_table.py
â”‚   â”‚   â”‚   â”œâ”€â”€ system_prompt_table.py
â”‚   â”‚   â”‚   â””â”€â”€ user_prompt_table.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ repositories
â”‚   â”‚   â”œâ”€â”€ ai_agent_repository.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_prompt_response_repository.py
â”‚   â”‚   â”œâ”€â”€ system_prompt_repository.py
â”‚   â”‚   â””â”€â”€ user_prompt_repository.py
â”‚   â”œâ”€â”€ services
â”‚   â”‚   â”œâ”€â”€ process_hugging_face_ai_prompt.py
â”‚   â”‚   â”œâ”€â”€ process_huggingface_ai_response.py
â”‚   â””â”€â”€ utils
â”‚       â”œâ”€â”€ db_operation_type.py
â”‚       â”œâ”€â”€ error_messages.py
â”‚       â”œâ”€â”€ field_descriptions.py
â”‚       â”œâ”€â”€ get_base_url.py
â”‚       â”œâ”€â”€ hugging_face_ai_model_enum.py
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger_info_messages.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â”œâ”€â”€ log_initializer.py
â”‚       â”œâ”€â”€ logs_re_namer.py
â”‚       â”œâ”€â”€ saved_sql_query
â”‚       â””â”€â”€ success_messages.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```
## The way I designed the project's flow 
### My design philosophy
This project prioritizes correctness over cleverness.

I intentionally avoided premature optimizations (Redis, queues) and focused on:
- clear flow
- strong invariants
- observable behavior (logs + timings)

### Repositories 
```bash
â”œâ”€â”€ repositories
â”‚   â”‚   â”œâ”€â”€ ai_agent_repository.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ system_prompt_repository.py
â”‚   â”‚   â””â”€â”€ user_prompt_repository.py
```
The way I designed my repositories:
- These repositories will be responsible for carrying out simple CRUD or complex database queries only
- It return structured result objects
- It also handles the database level exceptions and then pass those exceptions back to the controller layer.
- It own exactly one table in the database and does not know about other tables in the database
- It validate inputs for persistence

### Controllers
```bash
â”‚   â”œâ”€â”€ controllers
â”‚   â”‚   â”œâ”€â”€ agent_controllers.py
â”‚   â”‚   â”œâ”€â”€ hugging_face_ai_model_controllers.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ prompt_controllers.py
```
The way I designed my controllers : 
- They do not know SQLAlchemy
- They do not talk to the database
- They do not know about hugging face 
- They are only responsible to orchestrate the services and repositories according to the business logic 
- They translate the result after processing comming from the services and repositories into HTTP 

I made sure that : 
- Repositories return RepositoryClassResponse
- Repositories do not raise HTTPException
- Repositories do not know about FastAPI
- Repositories donâ€™t mix multiple tables casually

### services
```bash
â”‚   â”œâ”€â”€ services
â”‚   â”‚   â”œâ”€â”€ process_huggingface_ai_response.py
â”‚   â”‚   â”œâ”€â”€ process_prompt.py
```
I made this service layer to:
- Orchestrate multiple repositories
- Call external systems
- Implement workflows
- Enforce business sequencing
- Handle retries/timeouts
- Return domain-level results

### apis
```bash
â”‚   â”œâ”€â”€ apis
â”‚   â”‚   â”œâ”€â”€ agent_api.py
â”‚   â”‚   â”œâ”€â”€ hugging_face_api.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ prompt_apis.py
```
What I wanted this layer to do:
- define HTTP routes
- bind request/response models
- do dependency injection
- log request metadata
- forward the call to a controller

They do not:
- implement business rules
- talk to repositories
- talk to Hugging Face
- contain workflow logic

Each API file corresponds to:
- a domain concept
- a controller
- a use-case family

API modules define HTTP contracts and routing. They delegate all behavior to controllers and services and remain free of business logic.

### database 
```bash
â”‚   â”œâ”€â”€ database
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ db_session.py
â”‚   â”‚   â”œâ”€â”€ db_transaction_exception_handler.py
```
In this project I intentionally keep database access simple, explicit, and predictable.

Rather than hiding database behavior behind abstractions, I chose the design that makes transaction boundaries and failure behavior obvious to anyone reading the code.

#### **Single source of database access**
The project exposes one canonical way to access the database.
- A single SQLAlchemy engine
- A single session factory
- A single dependency (get_db) used everywhere

This ensures:
- consistent connection pooling
- predictable session lifecycle
- no accidental multiple engines or sessions

All database access flows through one controlled entry point to avoid hidden state and connection leaks.

Transactions are not started implicitly inside repositories.

Instead:
- transactions are started at the service or controller level
- repositories assume a valid session already exists
Rule of thumb is 
- Repositories describe what to write.
- Services decide when it is safe to commit.

Why this matters:
- external calls (like Hugging Face) are never executed inside DB transactions
- slow or failing network calls cannot lock database resources
- rollback behavior is deterministic

Failure-driven rollback using exceptions
- carries a structured domain response
- automatically triggers a rollback when raised inside a transaction block
- avoids mixing HTTP concerns into database logic

# Project's working
## How this project works?
- User sends a prompt to the API
- System fetches the agentâ€™s system prompt
- Prompt is sent to Hugging Face LLM
- Response is validated
- Only after success, both:
    - user prompt
    - LLM response are saved inside a single DB transaction
- If anything fails â†’ automatic rollback
## What I prevented?
- No partial writes if LLM fails
- No DB locks during slow network calls
- No inconsistent promptâ€“response pairs
- Clear failure boundaries
- Production-safe transaction handling

## Hugging face LLM context management mechanism
### Problem I am trying to solve
Large Language Models (LLMs) accessed via the Hugging Face Inference API are stateless by default.
Each request is processed independently, meaning the model does not â€œrememberâ€ previous interactions unless conversation history is explicitly sent with every request.

For an internal, agent-based platform where:
- multiple employees build their own AI agents,
- agents are expected to hold conversational continuity,
- performance and simplicity are critical,

I needed a reliable, predictable, and controllable way to maintain context without introducing unnecessary complexity or latency.

### I went with **Sliding Window Context**
I implemented server-side sliding window context management, where:
- All user prompts and assistant responses are persisted in a database.
- For each new user request, a bounded subset of recent conversation turns is reconstructed and sent to the LLM.
- Older context is automatically dropped when token limits are reached.

This approach ensures:
- Deterministic behavior
- Full control over what the model sees
- Compatibility with any Hugging Face chat-capable model

### Why This Approach?
#### Why NOT rely on model-side memory?
- Hugging Face API does not provide persistent conversational memory
- Model state cannot be trusted across requests
- Scaling agents would require session pinning (bad for performance)
#### Why NOT store the entire conversation every time?
- Token limits would be exceeded quickly
- Cost and latency increase linearly with conversation length
- Older messages often lose relevance
#### Why Sliding Window?
- Sliding window context is:
- Simple to reason about
- Predictable under load
- Easy to debug
- Model-agnostic
- Safe for internal enterprise use
#### High level LLM context management system architecture
```bash
User Request
   â†“
Fetch System Prompt
   â†“
Fetch Conversation History
   â†“
Context Builder (token-bounded)
   â†“
Hugging Face LLM
   â†“
Persist Response
```
### Context Window Construction Logic
The context is built using three components:
#### System Prompt (Always Included)
The system prompt defines:
- Agent behavior
- Safety constraints
- Output format expectations

This is always injected as the first message.

#### Conversation History (Sliding Window)
Conversation turns are stored as:
- user messages
- assistant messages

The builder:
- Iterates from most recent â†’ oldest
- Adds messages until the token budget is exhausted
- Stops cleanly when the budget is reached

Older messages are intentionally dropped, not truncated.

#### New User Prompt
The current user input is always appended last, ensuring:
- The model responds to the latest intent
- Context does not overshadow the new request

#### Token budget strategy
| Purpose                     | Tokens |
| --------------------------- | ------ |
| Total allowed               | 3000   |
| Reserved for model response | 800    |
| Available for history       | 2200   |

This prevents:
- Model cut-offs
- Partial responses
- Unpredictable truncation by the provider

### Current Token Counting Strategy (v1)
At present, token usage is estimated using a naive word-based counter:
```python
len(text.split())
```
#### Why this is acceptable (for now):
- Fast
- Zero dependencies
- Good enough for early-stage internal tooling
- Keeps the architecture flexible

#### Known Limitations (Intentional)
We explicitly acknowledge the following limitations:
- No long-term memory
    - Facts like names or roles are not permanently remembered
    - Once context slides out, the model forgets
- Naive token estimation
    - Actual tokenizer behavior varies by model
    - This may slightly over/under-estimate capacity
- No semantic relevance scoring
    - Messages are included purely by recency
    - Not by importance or topic relevance

These are design trade-offs, not oversights.

### Improvement Token Counting Strategy (v2)
Using **Hugging Face tokenizer (model-specific)**

This will allow:
- Precise budgeting
- Model-aware limits

#### Issues I faced when implementing Hugging Face tokenizer
What is actually failing (root cause) : 
```python
AutoTokenizer.from_pretrained(
    model_name,
    use_fast=True
)
```
```python
model_name = "meta-llama/Llama-3.1-8B-Instruct"
```
Why this fails? 

```meta-llama/Llama-3.1-8B-Instruct``` is a gated Hugging Face model.

That means:
- I must explicitly accept Metaâ€™s license
- I must authenticate when downloading anything related to it
    - config.json
    - tokenizer
    - vocab
    - merges

My HF inference API call may still work (because I passed a Bearer token),
but AutoTokenizer.from_pretrained() is a separate HF Hub call.

Tokenizer loading does not automatically reuse my inference token.

#### Solution
**Authenticate Hugging Face ONCE at system level** <br>
Run this on the server / dev machine: <br>
```bash
huggingface-cli login
```
```bash
(venv) ğŸ base î‚° aditya@aditya-IdeaPad-5-15ITL05 î‚° ~/github/agentic_ai_cloud î‚° î‚  optimization/sliding_context_window_management Â± î‚° huggingface-cli login

âš ï¸  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.

    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|
    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|
    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|
    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|
    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|

    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .
Enter your token (input will not be visible): 
Add token as git credential? (Y/n) y
Token is valid (permission: fineGrained).
The token `fast-api` has been saved to /home/aditya/.cache/huggingface/stored_tokens
Cannot authenticate through git-credential as no helper is defined on your machine.
You might have to re-authenticate when pushing to the Hugging Face Hub.
Run the following command in your terminal in case you want to set the 'store' credential helper as default.

git config --global credential.helper store

Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.
Token has not been saved to git credential helper.
Your token has been saved to /home/aditya/.cache/huggingface/token
Login successful.
The current active token is: `fast-api`
```
You may encounter this git related error when setting up this project in your local machine. Run the command below
```bash
(venv) ğŸ base î‚° aditya@aditya-IdeaPad-5-15ITL05 î‚° ~/github/agentic_ai_cloud î‚° î‚  optimization/sliding_context_window_management Â± î‚° git config --global credential.helper store
```
Try again with this command 
```bash
(venv) ğŸ base î‚° aditya@aditya-IdeaPad-5-15ITL05 î‚° ~/github/agentic_ai_cloud î‚° î‚  optimization/sliding_context_window_management Â± î‚° huggingface-cli login
```
The above command stores the auth_token for the hugging face api in this location you can use the command below to verify if the token is stored successfully or not
```bash
cat ~/.cache/huggingface/token

hf_KiMIsnUpKaUDZvtD1234567890@#ELsmib%                                                                               
```
Now all of these should work automatically:
- AutoTokenizer.from_pretrained(...)
- AutoConfig.from_pretrained(...)
- Gated repos
- Private models

This is what every serious HF deployment does.

Now that I am logged in using hugging face cli 
- I accepted the terms and conditions for accessing this model repository
    - Every company has its own terms and conditions that you will have to agree on 
- After agreeing to the terms and conditions I have to reuest access
![requesting_access_to_llm_model_hf](docs_images/requesting_access_to_llm_model_hf.png)
- After the request is granted you will see something like this 
![repo_reuest_granted](docs_images/repo_reuest_granted.png)
- After this hugging face tokenizer can be used for counting tokens accurately without any errors





### Future Improvements (Planned, Not Premature)
This design is intentionally extensible. Planned upgrades include:
#### Accurate Token Counting
Replace the naive counter with:
- Hugging Face tokenizer (model-specific)
- or tiktoken for OpenAI-compatible models

#### Structured Long-Term Memory
Introduce a separate agent memory store for:
- Facts (â€œAditya is a Python developerâ€)
- Preferences
- Roles

This memory will be:
- Explicitly injected
- Curated
- Not dependent on conversation recency

#### Relevance-Based Context Selection
Instead of pure recency:
- Score messages by semantic similarity
- Include fewer but more relevant turns
- Reduce noise in long conversations

#### Observability Enhancements
- Per-message token usage logs
- Context diffs between requests
- Debug flags to inspect dropped messages

### Tool orchestration 
For LLm to be able to use the tools for agentic work I chose MCP server 
```bash
Application
 â”œâ”€ Agent config (DB)
 â”œâ”€ system_prompt (DB)
 â”œâ”€ model selector
 â”œâ”€ MCP Client
 â”‚    â””â”€ MCP Server
 â”‚         â”œâ”€ search tool
 â”‚         â”œâ”€ email tool
 â”‚         â”œâ”€ db tool
 â”‚         â””â”€ filesystem tool
 â””â”€ Model (Claude / Llama / etc...)

```
What is MCP? 

MCP is a Model Context Protocol. MCP standardizes how an LLM runtime talks to external tools + data sources.

MCP defines:
- how tools are exposed
- how context is injected
- how models request actions
- how responses are structured

MCP server fits this project's use case and what I am trying to do
- per-agent system prompts
- per-agent tool allow/deny
- per-agent model selection
- strong control (what model can/canâ€™t do)
- persistence (DB)
- HF compatibility
- future multi-model support